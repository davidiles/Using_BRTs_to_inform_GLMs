---
title: "Using BRTs to improve Bayesian species distribution models"
author: "David Iles"
date: "Generated on `r Sys.Date()`"
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "png",
  dev.args = list(type = "cairo-png"),
  fig.width = 6,
  fig.height = 4,
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  knitr::opts_knit$set(root.dir = 'C:/Users/IlesD/OneDrive - EC-EC/Iles/Projects/Landbirds/Using_BRTs_to_inform_GLMs/')
)
```

# Motivation

We want to test if Machine Learning methods and Bayesian models can be used in combination to produce better estimates of species distributions. Our approach is to:

1) Fit Boosted Regression Trees (BRTs) to a dataset, which identify the most important covariates for predicting species distributions, then

2) Use the most important covariates in a Bayesian species distribution model that allows us to propagage uncertainty, and simultaneously accounts for spatial autocorrelation, random effects, and informative priors.

This approach may be an efficient way to "let the data speak for itself" in identifying the most important covariates through machine learning, after which we can use Bayesian models to properly account for spatial autocorrelation and error propagation.  On the other hand, this may be a form of "double-dipping" from the data that will lead to over-fit models that reduce performance.  

We therefore conducted simulations to evaluate if this is a reasonable approach for modeling.

# Background

Boosted regression tree (BRT) approaches, and machine learning more generally, are extremely good at identifying important covariates for predicting species distributions.  They naturally accommodate complex, non-linear, and interacting response functions among multiple covariates, and do not suffer from problems with variable collinearity.  

However, BRTs cannot include spatial covariation (when the response variable at a location is similar to the response variable at nearby locations, after accounting for covariates).  Additionally, BRTs are not well-suited to the inclusion of random effects (e.g., repeated-measures data), and cannot include integrated models (e.g., where multiple response variables with different error distributions are affected by a shared process). It can also be difficult to properly propagate uncertainty with BRTs, which is critical for species status and trend assessments.

In contrast, Bayesian models are excellent for describing and propagating multiple sources of uncertainty.  They can also account for spatial autocorrelation, and can include 'informative priors' to help improve model outputs.  However, Bayesian models typically cannot accommodate large numbers of covariates, and can suffer from lack of parameter identifiability when multiple covariates are correlated with each other. 

# Methods

We used simulations to examine whether a two-stage approach to fitting models improves species distribution estimates, compared to just fitting a BRT.

## Simulation example

```{r load_libraries, echo=FALSE}

# ----------------------------------------
# Libraries
# ----------------------------------------

library(ggplot2)
library(dplyr)
library(tidyr)
library(viridis)
library(faux)  # for simulating correlated random variables
library(dismo) # for fitting BRTs
library(fields)# for simulating random covariates
library(inlabru)
library(INLA)
library(sf)
library(ggpubr)

rm(list=ls())

```

Since almost all habitat and environmental covariates are spatially autocorrelated, we simulated 25 spatially autocorrelated random variables, each with different properties.  Some covariates are very noisy, some are smooth, some are highly spatially autocorrelated, and others have little spatial autocorrelation.

The plots below illustrate each of the covariates.

```{r simulate_landscapes, cache=TRUE, fig.width=10, fig.height=10, echo = FALSE}

set.seed(555)

# ----------------------------------------
# Simulate landscape with 25 spatially autocorrelated covariates
# ----------------------------------------

n_var <- 25
simdat <- expand.grid(Lat = seq(0,100), Lon = seq(0,100))

plot_list <- list()

for (i in 1:n_var){
  grid <- list(x = seq(0,100), y = seq(0,100))
  simcov <- NULL
  while(is.null(simcov)){
    
    simulate_landscape <- function(){
      tryCatch(expr = {
        obj1 <- matern.image.cov(grid=grid, aRange = runif(1,1,20) , smoothness = runif(1,0.1,1), setup=TRUE)
        simcov <- sim.rf(obj1) %>% reshape2::melt()
        
        },
        error = function(e){NULL})
    }
    simcov <- simulate_landscape()
    if ("try-error" %in% class(simcov)) simcov <- NULL
  }
  
  
  simdat <- cbind(simdat,simcov$value)
  varname <- colnames(simdat)[2+i] <- paste0("Cov_",i)
  
  cov_plot <- ggplot()+
    geom_raster(data = simdat, aes(x = Lon, y = Lat, fill = .data[[varname]]))+
    scale_fill_gradientn(colors = viridis(10), guide = "none")+
    ggtitle(paste0("Cov #",i))+
    theme_bw()
  
  plot_list[[i]] <- cov_plot
  
}

cov_plot_all <- ggarrange(plotlist = plot_list, ncol = 5, nrow = 5, align = "hv") %>%
  annotate_figure(., top = text_grob("Simulated spatial covariates", color = "black", face = "bold", size = 14))
print(cov_plot_all)

```

In our simulations, we allow the first 10 covariates to influence the species distribution.  We draw a random 'effect size coefficient' for each variable, then create the species distribution by multiplying each spatial covariate by its effect size, and adding them together.  The resulting species distribution is shown below:

```{r species_distribution, fig.width=6, fig.height=6, echo = FALSE}

# Covariate effects
coefs <- rep(0,n_var)
coefs[1:10] <- runif(10,-1,1) # Beta coefficients for first 10 covariates
coefs <- matrix(coefs,ncol=1)

# Response variable
y <- as.matrix(simdat[,3:ncol(simdat)]) %*% coefs

simdat <- simdat %>% mutate(y = y) %>% relocate(Lat,Lon,y)

# Map of response variable
response_plot <- ggplot(simdat)+
  geom_raster(aes(x = Lon, y = Lat, fill = y))+
  scale_fill_gradientn(colors = viridis(10))+
  theme_bw()+
  ggtitle("Simulated response variable\n(e.g., a species distribution)\n")

print(response_plot)

```

We then simulate a survey of the species across the landscape.  We assume there were 250 surveys conducted.  At each survey location, the response variable is recorded, along with covariate information.  

However, we assume that the surveyor does not record the first 3 covariates.  Thus, there are several spatially autocorrelated covariates that are affecting the species distribution but which cannot be (or have not been) measured.  

Below, we illustrate the survey locations across the landscape, and the table shows the first several rows of data available for analysis:

```{r analysis_data_example, echo = FALSE, fig.width=6, fig.height=6}

# Assume we cannot measure three covariates (drop them from dataframe)
simdat <- simdat %>% dplyr::select(-Cov_1,-Cov_2,-Cov_3)

# ----------------------------------------
# Select 500 survey locations
# ----------------------------------------

n_survey <- 500

dat <- sample_n(simdat, n_survey, replace = TRUE)

# Add observation error
dat$y <- dat$y + rnorm(nrow(dat),0,0.1)

survey_plot <- ggplot(simdat)+
  geom_raster(aes(x = Lon, y = Lat, fill = y))+
  scale_fill_gradientn(colors = viridis(10))+
  geom_jitter(data = dat, aes(x = Lon, y = Lat))+
  theme_bw()+
  ggtitle("Survey locations")
print(survey_plot)

print(head(dat))

```

We first analyze the dataset with boosted regression trees, using the following code.  

```{r fit_brt, cache = TRUE, results = "hide",fig.show='hide', fig.width=6, fig.height=20}

# ---------------------------------------
# Fit BRT and generate landscape predictions
# ---------------------------------------

brt <- gbm.step(data=dat, gbm.x = 4:ncol(dat), gbm.y = 3,
                family = "gaussian", tree.complexity = 5,
                learning.rate = 0.01, bag.fraction = 0.5,
                verbose = FALSE,
                plot.main = FALSE)

# generate predictions from brt across landscape
pred_brt <- predict(brt, simdat,n.trees=brt$gbm.call$best.trees, type="response")

# variable importance
var_imp <- summary(brt)
```

Variable importance scores can be extracted from boosted regression tree analyses using the `summary` command.  A plot of relative variable importance is shown below:

```{r var_importance, echo = FALSE, results = "hide",fig.width=6, fig.height=6}

var_imp$var <- factor(var_imp$var, levels = rev(var_imp$var))
var_imp_plot <- ggplot(var_imp, aes(x = rel.inf, y = var))+
  geom_bar(stat = "identity")+
  xlab("Relative importance")+
  ylab("Variable name")+
  ggtitle("Relative variable importance based on BRT")+
  theme_bw()

print(var_imp_plot)
```

The top 5 most important variables (in this case `r var_imp$var[1:5]`) were then included in a Bayesian species distribution model, fit using the `inlabru` package in R.

The model includes a spatially autocorreled random field to account for spatial autocorrelation that is not attributable to the measured covariates.

```{r INLA, cache = TRUE}

# ---------------------------------------
# Fit model using INLA
# ---------------------------------------

# USE TOP 5 MOST IMPORTANT VARIABLES FROM BRT
top_vars <- var_imp$var[1:5]

# covert data to spatial object
simdat_sf <- st_as_sf(simdat, coords = c("Lon","Lat"),remove = FALSE)
dat_sf <- st_as_sf(dat, coords = c("Lon","Lat"),remove = FALSE)

# make a two extension hulls and mesh for spatial model
hull <- fm_extensions(simdat_sf)

# Spatial mesh
mesh_spatial <- fm_mesh_2d_inla(
  boundary = hull, 
  max.edge = c(5, 10),
  cutoff = 2
)

# Controls the 'residual spatial field'
prior_range <- c(1, 0.1)   # 10% chance range is smaller than 1
prior_sigma <- c(1,0.1)    # 10% chance sd is larger than 1
matern_coarse <- inla.spde2.pcmatern(mesh_spatial,
                                     prior.range = prior_range, 
                                     prior.sigma = prior_sigma
)

# Priors/shrinkage on covariate effects
sd_linear <- 0.1  
prec_linear <-  c(1/sd_linear^2,1/(sd_linear/2)^2)

# Model formula
model_components = as.formula(paste0('~
            Intercept(1)+
            spde_coarse(main = geometry, model = matern_coarse)+',
            paste0("Beta1_",top_vars,'(1,model="linear", mean.linear = 0, prec.linear = ', prec_linear[1],')', collapse = " + ")))

model_formula= as.formula(paste0('y ~
                  Intercept +
                  spde_coarse +',
                  paste0("Beta1_",top_vars,'*',top_vars, collapse = " + ")))

fit_INLA <- NULL
while(is.null(fit_INLA)){
  
  fit_model <- function(){
    tryCatch(expr = {bru(components = model_components,
                         like(family = "gaussian",
                              formula = model_formula,
                              data = dat_sf),
                         
                         options = list(control.compute = list(waic = FALSE, cpo = FALSE),
                                        bru_verbose = 4))},
             error = function(e){NULL})
  }
  fit_INLA <- fit_model()
  
  if ("try-error" %in% class(fit_INLA)) fit_INLA <- NULL
}

# Prediction
pred_formula = as.formula(paste0(' ~
                  Intercept +
                  spde_coarse +',paste0("Beta1_",top_vars,'*',top_vars, collapse = " + ")))

pred_inla <- generate(fit_INLA,
                      simdat_sf,
                      formula =  pred_formula,
                      n.samples = 1000)

# Mean predicted response for every pixel on the landscape
pred_inla <- apply(pred_inla,1,mean)
```

Below, we compare the 'true' response surface (y) with the predicted response surfaces estimated from either Boosted Regression Trees (BRTs) or with the Bayesian model (INLA).

```{r compare_BRT_INLA, cache = TRUE, echo=FALSE,fig.width=20, fig.height=5}
simdat$pred_brt <- pred_brt
simdat$pred_inla <- pred_inla

lim <- range(simdat[,c("y","pred_brt","pred_inla")])
plot_y <- ggplot(simdat)+
  geom_raster(aes(x = Lon, y = Lat, fill = y))+
  scale_fill_gradientn(colors = viridis(10), limits = lim, name = "y")+
  theme_bw()+
  ggtitle("True response surface")

plot_brt <- ggplot(simdat)+
  geom_raster(aes(x = Lon, y = Lat, fill = pred_brt))+
  scale_fill_gradientn(colors = viridis(10), limits = lim, name = "BRT\nestimate")+
  theme_bw()+
  ggtitle("BRT estimate")

plot_inla <- ggplot(simdat)+
  geom_raster(aes(x = Lon, y = Lat, fill = pred_inla))+
  scale_fill_gradientn(colors = viridis(10), limits = lim, name = "INLA\nestimate")+
  theme_bw()+
  ggtitle("INLA estimate")

compare_plots <- ggarrange(plot_y, plot_brt, plot_inla, nrow=1, align = "hv")
print(compare_plots)
```


To evaluate which prediction surface is a better representation of the 'true' response surface on the left, we calculate two metrics: 1) the root mean squared error between the prediction and the true surface, and 2) the correlation between the prediction surface and the true surface.

```{r rmse_cor}
RMSE_brt <- sqrt(mean((simdat$pred_brt - simdat$y)^2))
RMSE_inla <- sqrt(mean((simdat$pred_inla - simdat$y)^2))
cor_brt <- cor(simdat$pred_brt,simdat$y)
cor_inla <- cor(simdat$pred_inla,simdat$y)
```

The RMSE for the BRT surface is `r round(RMSE_brt,2)`, while the RMSE for the INLA surface is `r round(RMSE_inla,2)`. Thus, in this simulation, the RMSE from the Bayesian model was `r -round(100*(RMSE_inla- RMSE_brt)/RMSE_brt,2)`% lower than the model fit using BRT.

Additionally, the correlation between the true surface and the prediction from BRT was `r round(cor_brt,2)`.  The correlation for the Bayesian model prediction was `r round(cor_inla,2)`.

These results (as well as visual inspection of the surfaces above) illustrate that the Bayesian model resulted in much better predictions of the species distribution than the boosted regression tree.

## Repeated simulations

Since the result above might have been weird and/or unrepresentative, we need to re-run the simulation many times to see how consistently a Bayesian model improves the model fit.  

We conducted 500 repeated simulations.  For each simulation, we stored the Root Mean Squared Error and correlation between model predictions and the 'true' response surface (y). We stored these values separately based on either BRT models, or Bayesian models that incorporated the best covariates identified by BRTs.

Results are illustrated below.

```{r conduct_repeated_simulations, echo = FALSE}

results <- data.frame()

for (rep in 1:500){
  
  # Load if file already exists
  if (file.exists("output/results.RDS")) results <- readRDS("output/results.RDS")
  
  if (nrow(results)>=500) break
  
  # ----------------------------------------
  # Simulate landscape with 25 spatially autocorrelated covariates
  # ----------------------------------------
  
  n_var <- 25
  simdat <- expand.grid(Lat = seq(0,100), Lon = seq(0,100))
  
  for (i in 1:n_var){
    grid <- list(x = seq(0,100), y = seq(0,100))
    obj1 <- matern.image.cov(grid=grid, aRange = runif(1,1,10) , smoothness = runif(1,0.5,1), setup=TRUE)
    simcov <- sim.rf(obj1) %>% reshape2::melt()
    simdat <- cbind(simdat,simcov$value)
  }
  
  colnames(simdat)[3:ncol(simdat)] <- paste0("Cov_",1:n_var)
  
  # ----------------------------------------
  # Simulate response variable, which depends on first 10 covariates
  # ----------------------------------------
  
  # Covariate effects
  coefs <- rep(0,n_var)
  coefs[1:10] <- runif(10,-1,1) # Beta coefficients for first 10 covariates
  coefs <- matrix(coefs,ncol=1)
  
  # Response variable
  y <- as.matrix(simdat[,3:ncol(simdat)]) %*% coefs
  
  simdat <- simdat %>% mutate(y = y) %>% relocate(Lat,Lon,y)
  
  # Map of response variable
  ggplot(simdat)+
    geom_raster(aes(x = Lon, y = Lat, fill = y))+
    scale_fill_gradientn(colors = viridis(10))+
    theme_bw()
  
  # Assume we cannot measure three covariates (drop them from dataframe)
  simdat <- simdat %>% dplyr::select(-Cov_1,-Cov_2,-Cov_3)
  
  # ----------------------------------------
  # Select 500 survey locations
  # ----------------------------------------
  
  n_survey <- 500
  
  dat <- sample_n(simdat, n_survey, replace = TRUE)
  
  # Add observation error
  dat$y <- dat$y + rnorm(nrow(dat),0,0.1)
  
  ggplot(simdat)+
    geom_raster(aes(x = Lon, y = Lat, fill = y))+
    scale_fill_gradientn(colors = viridis(10))+
    geom_jitter(data = dat, aes(x = Lon, y = Lat))+
    theme_bw()
  
  # ---------------------------------------
  # Fit BRT and generate landscape predictions
  # ---------------------------------------
  
  brt <- gbm.step(data=dat, gbm.x = 4:ncol(dat), gbm.y = 3,
                  family = "gaussian", tree.complexity = 5,
                  learning.rate = 0.01, bag.fraction = 0.5)
  
  # predictions from brt across landscape
  pred_brt <- predict(brt, simdat,n.trees=brt$gbm.call$best.trees, type="response")
  
  # variable importance
  #var_imp <- summary(brt)
  
  # ---------------------------------------
  # Fit model using INLA
  # ---------------------------------------
  
  # USE TOP 5 MOST IMPORTANT VARIABLES FROM BRT
  top_vars <- var_imp$var[1:5]
  
  # covert data to spatial object
  simdat_sf <- st_as_sf(simdat, coords = c("Lon","Lat"),remove = FALSE)
  dat_sf <- st_as_sf(dat, coords = c("Lon","Lat"),remove = FALSE)
  
  # make a two extension hulls and mesh for spatial model
  hull <- fm_extensions(
    simdat_sf
  )
  
  # Spatial mesh
  mesh_spatial <- fm_mesh_2d_inla(
    boundary = hull, 
    max.edge = c(5, 10),
    cutoff = 2
  )
  
  # Controls the 'residual spatial field'.  This can be adjusted to create smoother surfaces.
  prior_range <- c(1, 0.1)   # 10% chance range is smaller than 1
  prior_sigma <- c(1,0.1)    # 10% chance sd is larger than 1
  matern_coarse <- inla.spde2.pcmatern(mesh_spatial,
                                       prior.range = prior_range, 
                                       prior.sigma = prior_sigma
  )
  
  # How much shrinkage should be applied to covariate effects?
  sd_linear <- 0.1  
  prec_linear <-  c(1/sd_linear^2,1/(sd_linear/2)^2)
  
  # Model formula
  model_components = as.formula(paste0('~
            Intercept(1)+
            spde_coarse(main = geometry, model = matern_coarse)+',
            paste0("Beta1_",top_vars,'(1,model="linear", mean.linear = 0, prec.linear = ', prec_linear[1],')', collapse = " + ")))
  
  model_formula= as.formula(paste0('y ~
                  Intercept +
                  spde_coarse +',
                  paste0("Beta1_",top_vars,'*',top_vars, collapse = " + ")))
  
  fit_INLA <- NULL
  while(is.null(fit_INLA)){
    
    fit_model <- function(){
      tryCatch(expr = {bru(components = model_components,
                           like(family = "gaussian",
                                formula = model_formula,
                                data = dat_sf),
                           
                           options = list(control.compute = list(waic = FALSE, cpo = FALSE),
                                          bru_verbose = 4))},
               error = function(e){NULL})
    }
    fit_INLA <- fit_model()
    
    if ("try-error" %in% class(fit_INLA)) fit_INLA <- NULL
  }
  
  # Prediction
  pred_formula = as.formula(paste0(' ~
                  Intercept +
                  spde_coarse +',paste0("Beta1_",top_vars,'*',top_vars, collapse = " + ")))
  
  # Note that predictions are initially on log scale
  pred_inla <- generate(fit_INLA,
                        simdat_sf,
                        formula =  pred_formula,
                        n.samples = 1000)
  
  pred_mean_inla <- apply(pred_inla,2,mean)
  
  pred_inla <- apply(pred_inla,1,mean)
  
  # ---------------------------------------
  # Evaluate quality of model fits; save in results dataframe
  # ---------------------------------------
  
  RMSE_brt <- sqrt(mean((pred_brt - simdat$y)^2))
  RMSE_inla <- sqrt(mean((pred_inla - simdat$y)^2))
  
  results <- rbind(results,data.frame(RMSE_brt = RMSE_brt,
                                      RMSE_inla = RMSE_inla,
                                      cor_brt = cor(pred_brt,simdat$y),
                                      cor_inla = cor(pred_inla,simdat$y)
  ))
  
  # Save results
  saveRDS(results,"output/results.RDS")
  
}

# ---------------------------------------
# Load results
# ---------------------------------------

results <- readRDS("output/results.RDS")

# ---------------------------------------
# Summarize results; Are GLM predictions better than BRTs?
# ---------------------------------------
results$simulation_number <- 1:nrow(results)

# How much does GAM reduce RMSE?
results$percent_reduction_RMSE <- 100*(results$RMSE_inla - results$RMSE_brt)/results$RMSE_brt

RMSE_plot <- ggplot(data = results, aes(x = percent_reduction_RMSE))+
  geom_histogram(fill = "dodgerblue")+
  geom_vline(xintercept = 0, linetype = 2)+
  ylab("Frequency\n(number of simulations)")+
  xlab("Percent reduction in Root Mean Squared Error\n(When fitting INLA after a BRT)")+
  ggtitle("Does INLA reduce Root Mean Squared Error, compared to BRT?")+
  theme_bw()
print(RMSE_plot)

cor_plot <- ggplot(data = results, aes(x = cor_inla - cor_brt))+
  geom_histogram(fill = "dodgerblue")+
  geom_vline(xintercept = 0, linetype = 2)+
  ylab("Frequency\n(number of simulations)")+
  xlab("Improvement in Correlation\n(When fitting INLA after a BRT)")+
  ggtitle("Does INLA improve correlation with 'true' density, compared to BRT?")+
  theme_bw()
print(cor_plot)

# Percent of simulations where INLA resulted in a lower RMSE:
#mean(results$RMSE_inla < results$RMSE_brt) # 0.97

# Percent of simulations where INLA resulted in a higher correlation:
#mean(results$cor_inla > results$cor_brt) # 0.97
```

The Bayesian model resulted in lower RMSE for `r 100*round(mean(results$RMSE_inla < results$RMSE_brt),2)`% of simulations, and the mean percent change in RMSE was `r round(mean(100*(results$RMSE_inla- results$RMSE_brt)/results$RMSE_brt),2)`%, when using Bayesian models compared to BRTs.

Additionally, predictions from the Bayesian model had a higher correlation with the true response surface in `r 100*round(mean(results$cor_inla > results$cor_brt),2)`% of simulations.  The mean change in correlation was +`r round(mean(results$cor_inla-results$cor_brt),2)`.

# Conclusions

This analysis suggests that a Bayesian spatial model outperforms a machine learning model for predicting spatial response surfaces.

In this case, we used the machine learning model to identify the best covariates to consider including in the Bayesian model.  Then, we used the Bayesian model to capture residual spatial autocorrelation in the response surface, beyond that which is explained by explicit covariates.

This approach holds promise for improving species distribution estimates.